---
title: "MLPaper"
format: pdf
---



## 

加载包



```{r}
if (!require(dplyr)) install.packages("dplyr")
if (!require("xgboost")) install.packages("xgboost")
if (!require("caret")) install.packages("caret")
if (!require("data.table")) install.packages("data.table")
if (!require("Matrix")) install.packages("Matrix")

library(dplyr)
library(ggplot2)
library(xgboost)
library(caret)
library(data.table)
library(Matrix)


```



读取数据



```{r}
raw_data <- read.csv("train.csv")
test <- read.csv("test.csv")
```



数据展示



```{r}
# 显示数据的结构
str(raw_data)

# 显示全部数据
print(raw_data)
```



数据清理



```{r}
# 删除PatientID和DoctorInCharge
raw_data$PatientID <- NULL
raw_data$DoctorInCharge <- NULL
```



将种族和edu拆分



```{r}
# 使用model.matrix创建指示变量并保留其他所有变量
tdata<- model.matrix(~ . + factor(Ethnicity) + factor(EducationLevel) - Ethnicity - EducationLevel - 1, 
                                     data = raw_data)

# 转换后的数据集的列名调整，以便更加清晰地表示每个变量
colnames(raw_data_transformed) <- gsub("factor(Ethnicity)", "Ethnicity", colnames(tdata))
colnames(raw_data_transformed) <- gsub("factor(EducationLevel)", "EducationLevel", colnames(tdata))

#输出数据集
write.csv(tdata, "tdata.csv", row.names = FALSE)

```



选择合适模型（我和Dennis尝试过各种 往上面堆）

//

//

//

XGboost



```{r}
# 读取数据
data <- read.csv("tdata.csv")

# 数据预处理
data$Diagnosis <- as.factor(data$Diagnosis)  # 确保 Diagnosis 是分类变量
y <- data$Diagnosis
X <- data[, setdiff(names(data), "Diagnosis")]  # 特征

# 将数据分为训练集和测试集
set.seed(123)
train_index <- createDataPartition(y, p = 0.8, list = FALSE)
train_data <- X[train_index, ]
train_label <- y[train_index]
test_data <- X[-train_index, ]
test_label <- y[-train_index]

# 转换为 XGBoost 格式
dtrain <- xgb.DMatrix(data = as.matrix(train_data), label = as.numeric(train_label) - 1)
dtest <- xgb.DMatrix(data = as.matrix(test_data), label = as.numeric(test_label) - 1)

# 模型训练
params <- list(
  objective = "binary:logistic",
  eval_metric = "logloss",
  booster = "gbtree",
  eta = 0.1,
  max_depth = 6,
  subsample = 0.8,
  colsample_bytree = 0.8
)

set.seed(123)
xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,
  watchlist = list(train = dtrain, test = dtest),
  early_stopping_rounds = 10
)

# 预测与评估
predictions <- predict(xgb_model, dtest)
predictions <- ifelse(predictions > 0.5, 1, 0)

# 准确率
accuracy <- mean(predictions == as.numeric(test_label) - 1)

# 混淆矩阵
confusion_matrix <- table(Predicted = predictions, Actual = as.numeric(test_label) - 1)

# Precision, Recall 和 F1-Score
TP <- confusion_matrix[2, 2]
FP <- confusion_matrix[2, 1]
FN <- confusion_matrix[1, 2]
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
f1 <- 2 * precision * recall / (precision + recall)

# 输出结果
print(paste("测试集准确率:", round(accuracy * 100, 2), "%"))
print("混淆矩阵:")
print(confusion_matrix)
print(paste("Precision:", round(precision, 2)))
print(paste("Recall:", round(recall, 2)))
print(paste("F1-Score:", round(f1, 2)))
```



使用网格搜索 (Grid Search) 优化参数



```{r}
# 读取数据
data <- read.csv("tdata.csv")
data$Diagnosis <- as.factor(data$Diagnosis)
y <- data$Diagnosis
X <- data[, setdiff(names(data), "Diagnosis")]

# 分割数据集（分层抽样）
set.seed(123)
train_index <- createDataPartition(y, p = 0.8, list = FALSE)
train_data <- X[train_index, ]
train_label <- as.numeric(y[train_index]) - 1  # 转换为 0/1
test_data <- X[-train_index, ]
test_label <- as.numeric(y[-train_index]) - 1

# 转换为 DMatrix 格式
dtrain <- xgb.DMatrix(data = as.matrix(train_data), label = train_label)
dtest <- xgb.DMatrix(data = as.matrix(test_data), label = test_label)

# 定义参数网格为列表
param_grid <- expand.grid(
  max_depth = c(3, 6, 9),
  eta = c(0.01, 0.1, 0.3),
  subsample = c(0.6, 0.8, 1),
  colsample_bytree = c(0.6, 0.8, 1),
  min_child_weight = c(1, 5, 10),
  gamma = c(0, 1, 5)
)

# 转换参数网格为列表格式
param_list <- split(param_grid, seq(nrow(param_grid)))

# 自定义评估函数
custom_eval <- function(params) {
  set.seed(123)  # 保持随机性一致
  model <- xgb.train(
    params = list(
      objective = "binary:logistic",
      eval_metric = "logloss",
      max_depth = params$max_depth,
      eta = params$eta,
      subsample = params$subsample,
      colsample_bytree = params$colsample_bytree,
      min_child_weight = params$min_child_weight,
      gamma = params$gamma,
      tree_method = "hist"  # 使用直方图算法
    ),
    data = dtrain,
    nrounds = 100,
    watchlist = list(test = dtest),
    early_stopping_rounds = 10,
    verbose = 0
  )
  # 返回验证集 Logloss
  return(model$best_score)
}

# 对每组参数进行评估
results <- sapply(param_list, custom_eval)

# 获取最优参数
best_index <- which.min(results)
best_params <- param_list[[best_index]]

# 输出最优参数和对应的评估结果
print(best_params)
print(paste("最佳 Logloss:", round(min(results), 4)))



```



xgboost优化后



```{r}
# 读取数据
data <- read.csv("tdata.csv")

# 数据预处理
data$Diagnosis <- as.factor(data$Diagnosis)  # 确保 Diagnosis 是分类变量
y <- data$Diagnosis
X <- data[, setdiff(names(data), "Diagnosis")]  # 特征

# 将数据分为训练集和测试集
set.seed(123)
train_index <- createDataPartition(y, p = 0.8, list = FALSE)
train_data <- X[train_index, ]
train_label <- as.numeric(y[train_index]) - 1  # 转换为 0/1
test_data <- X[-train_index, ]
test_label <- as.numeric(y[-train_index]) - 1

# 转换为 XGBoost 格式
dtrain <- xgb.DMatrix(data = as.matrix(train_data), label = train_label)
dtest <- xgb.DMatrix(data = as.matrix(test_data), label = test_label)

# 最佳参数（基于优化结果）
best_params <- list(
  objective = "binary:logistic",
  eval_metric = "logloss",
  tree_method = "hist",
  max_depth = 9,
  eta = 0.3,
  subsample = 0.6,
  colsample_bytree = 0.8,
  min_child_weight = 1,
  gamma = 0
)

# 模型训练
set.seed(123)
xgb_model <- xgb.train(
  params = best_params,
  data = dtrain,
  nrounds = 100,
  watchlist = list(train = dtrain, test = dtest),
  early_stopping_rounds = 10,
  verbose = 0
)

# 保存模型
xgb.save(xgb_model, "best_xgb_model.model")

# 预测与评估
predictions <- predict(xgb_model, dtest)
predictions <- ifelse(predictions > 0.5, 1, 0)

# 准确率
accuracy <- mean(predictions == test_label)

# 混淆矩阵
confusion_matrix <- table(Predicted = predictions, Actual = test_label)

# Precision, Recall 和 F1-Score
TP <- confusion_matrix[2, 2]
FP <- confusion_matrix[2, 1]
FN <- confusion_matrix[1, 2]
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
f1 <- 2 * precision * recall / (precision + recall)

# 输出结果
print(paste("测试集准确率:", round(accuracy * 100, 2), "%"))
print("混淆矩阵:")
print(confusion_matrix)
print(paste("Precision:", round(precision, 2)))
print(paste("Recall:", round(recall, 2)))
print(paste("F1-Score:", round(f1, 2)))

```



输出test



```{r}

# 加载待预测的测试数据集
raw_data <- read.csv("test.csv")

# 保留PatientID用于最终输出
patient_ids <- raw_data$PatientID

# 删除PatientID和DoctorInCharge
raw_data$PatientID <- NULL
raw_data$DoctorInCharge <- NULL

# 使用model.matrix创建指示变量并保留其他所有变量
tdata <- model.matrix(~ . + factor(Ethnicity) + factor(EducationLevel) - Ethnicity - EducationLevel - 1, 
                      data = raw_data)

# 转换后的数据集的列名调整，以便更加清晰地表示每个变量
colnames(tdata) <- gsub("factor\\(Ethnicity\\)", "Ethnicity", colnames(tdata))
colnames(tdata) <- gsub("factor\\(EducationLevel\\)", "EducationLevel", colnames(tdata))

# 加载之前保存的XGBoost模型
xgb_model <- xgb.load("best_xgb_model.model")

# 创建DMatrix对象用于预测
dtest <- xgb.DMatrix(data = tdata)

# 使用模型进行预测
predictions <- predict(xgb_model, dtest)
diagnosis <- ifelse(predictions > 0.5, 1, 0)

# 生成包含PatientID和Diagnosis的输出数据框
output_df <- data.frame(PatientID = patient_ids, Diagnosis = diagnosis)

# 保存预测结果为CSV文件
write.csv(output_df, "predicted_diagnosis.csv", row.names = FALSE)

# 输出提示
print("预测结果已保存到文件 'predicted_diagnosis.csv'")

```

